services:

  nim-llm:
    container_name: nim-llm-ms
    image: nvcr.io/nim/meta/llama-3.1-70b-instruct-pb24h2:1.3.4
    volumes:
      - type: bind
        source: /tmp    # Can swap "/tmp" to another location, like "nim_cache" (provided). Ensure WRITE permissions first! eg. chmod -R a+w nim_cache. 
        target: /opt/nim/.cache/
    user: "${USERID}"
    ports:
    - "8999:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NVIDIA_API_KEY}
    shm_size: 20gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: ${INFERENCE_GPU_COUNT:-all}
              device_ids: ['${LLM_MS_GPU_ID1:-2}', '${LLM_MS_GPU_ID2:-3}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8000/v1/health/ready')"]
      interval: 10s
      timeout: 20s
      retries: 100
    profiles: ["local"]

  nemoretriever-embedding-ms:
    container_name: nemoretriever-embedding-ms
    image: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:1.5.0
    volumes:
      - type: bind
        source: /tmp    # Can swap "/tmp" to another location, like "nim_cache" (provided). Ensure WRITE permissions first! eg. chmod -R a+w nim_cache. 
        target: /opt/nim/.cache/
    ports:
    - "9080:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NVIDIA_API_KEY}
    user: "${USERID}"
    shm_size: 16GB
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: ${INFERENCE_GPU_COUNT:-all}
              device_ids: ['${EMBEDDING_MS_GPU_ID:-5}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health/ready"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10m
    profiles: ["local"]

  nemoretriever-ranking-ms:
    container_name: nemoretriever-ranking-ms
    image: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:1.3
    volumes:
      - type: bind
        source: /tmp    # Can swap "/tmp" to another location, like "nim_cache" (provided). Ensure WRITE permissions first! eg. chmod -R a+w nim_cache. 
        target: /opt/nim/.cache/
    ports:
    - "1976:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NVIDIA_API_KEY}
    user: "${USERID}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health/ready"]
      interval: 10s
      timeout: 20s
      retries: 100
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: ${INFERENCE_GPU_COUNT:-all}
              device_ids: ['${RANKING_MS_GPU_ID:-5}']
              capabilities: [gpu]
    profiles: ["local"]

  page-elements:
    image: ${YOLOX_IMAGE:-nvcr.io/nim/nvidia/nemoretriever-page-elements-v2}:${YOLOX_TAG:-1.2.0}
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NVIDIA_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - NGC_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - CUDA_VISIBLE_DEVICES=0
      - NIM_TRITON_MODEL_BATCH_SIZE=${PAGE_ELEMENTS_BATCH_SIZE:-1}
      # NIM OpenTelemetry Settings
      - NIM_OTEL_SERVICE_NAME=page-elements
      - NIM_OTEL_TRACES_EXPORTER=otlp
      - NIM_OTEL_METRICS_EXPORTER=console
      - NIM_OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
      - NIM_ENABLE_OTEL=true
      # Triton OpenTelemetry Settings
      - TRITON_OTEL_URL=http://otel-collector:4318/v1/traces
      - TRITON_OTEL_RATE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['${YOLOX_MS_GPU_ID:-4}']
              capabilities: [gpu]
    runtime: nvidia
    profiles: ["local"]

  graphic-elements:
    image: ${YOLOX_GRAPHIC_ELEMENTS_IMAGE:-nvcr.io/nim/nvidia/nemoretriever-graphic-elements-v1}:${YOLOX_GRAPHIC_ELEMENTS_TAG:-1.2.0}
    ports:
      - "8003:8000"
      - "8004:8001"
      - "8005:8002"
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NVIDIA_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - NGC_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - CUDA_VISIBLE_DEVICES=0
      - NIM_TRITON_MODEL_BATCH_SIZE=${GRAPHIC_ELEMENTS_BATCH_SIZE:-1}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['${YOLOX_GRAPHICS_MS_GPU_ID:-4}']
              capabilities: [gpu]
    runtime: nvidia
    profiles: ["local"]

  table-structure:
    image: ${YOLOX_TABLE_STRUCTURE_IMAGE:-nvcr.io/nim/nvidia/nemoretriever-table-structure-v1}:${YOLOX_TABLE_STRUCTURE_TAG:-1.2.0}
    ports:
      - "8006:8000"
      - "8007:8001"
      - "8008:8002"
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NGC_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - CUDA_VISIBLE_DEVICES=0
      - NIM_TRITON_MODEL_BATCH_SIZE=${TABLE_STRUCTURE_BATCH_SIZE:-1}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids:  ['${YOLOX_TABLE_MS_GPU_ID:-4}']
              capabilities: [gpu]
    runtime: nvidia
    profiles: ["local"]

  paddle:
    image: ${PADDLE_IMAGE:-nvcr.io/nim/baidu/paddleocr}:${PADDLE_TAG:-1.2.0}
    shm_size: 2gb
    ports:
      - "8009:8000"
      - "8010:8001"
      - "8011:8002"
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NVIDIA_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - NGC_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids:  ['${PADDLE_MS_GPU_ID:-4}']
              capabilities: [gpu]
    runtime: nvidia
    profiles: ["local"]

  # Main ingestor server which is responsible for ingestion
  ingestor-server:
    container_name: ingestor-server
    image: nvcr.io/nvidia/blueprint/ingestor-server:${TAG:-2.0.0}
    build:
      # Set context to repo's root directory
      context: ../
      dockerfile: src/ingestor_server/Dockerfile
    # start the server on port 8082 with 4 workers for improved latency on concurrent requests.
    command: --port 8082 --host 0.0.0.0 --workers 1

    # Common customizations to the pipeline can be controlled using env variables
    environment:
      # Path to example directory relative to root
      EXAMPLE_PATH: 'src/ingestor_server'

      ##===Vector DB specific configurations===
      # URL on which vectorstore is hosted
      APP_VECTORSTORE_URL: "http://milvus:19530"
      # Type of vectordb used to store embedding supported type milvus
      APP_VECTORSTORE_NAME: "milvus"
      # Type of vectordb search to be used
      APP_VECTORSTORE_SEARCHTYPE: ${APP_VECTORSTORE_SEARCHTYPE:-"dense"} # Can be dense or hybrid
      # Boolean to enable GPU index for milvus vectorstore specific to nvingest
      APP_VECTORSTORE_ENABLEGPUINDEX: ${APP_VECTORSTORE_ENABLEGPUINDEX:-True}
      # Boolean to control GPU search for milvus vectorstore specific to nvingest
      APP_VECTORSTORE_ENABLEGPUSEARCH: ${APP_VECTORSTORE_ENABLEGPUSEARCH:-True}
      # vectorstore collection name to store embeddings
      COLLECTION_NAME: ${COLLECTION_NAME:-multimodal_data}

      ##===MINIO specific configurations===
      MINIO_ENDPOINT: "minio:9010"
      MINIO_ACCESSKEY: "minioadmin"
      MINIO_SECRETKEY: "minioadmin"

      NVIDIA_API_KEY: ${NVIDIA_API_KEY:?"NVIDIA_API_KEY is required"}

      ##===Embedding Model specific configurations===
      # url on which embedding model is hosted. If Nvidia hosted API is used
      APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL-"nemoretriever-embedding-ms:8000"}
      APP_EMBEDDINGS_MODELNAME: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}
      APP_EMBEDDINGS_DIMENSIONS: ${APP_EMBEDDINGS_DIMENSIONS:-2048}

      ##===NV-Ingest Connection Configurations=======
      APP_NVINGEST_MESSAGECLIENTHOSTNAME: ${APP_NVINGEST_MESSAGECLIENTHOSTNAME:-"nv-ingest-ms-runtime"}
      APP_NVINGEST_MESSAGECLIENTPORT: ${APP_NVINGEST_MESSAGECLIENTPORT:-7670}

      ##===NV-Ingest Extract Configurations==========
      APP_NVINGEST_EXTRACTTEXT: ${APP_NVINGEST_EXTRACTTEXT:-True}
      APP_NVINGEST_EXTRACTTABLES: ${APP_NVINGEST_EXTRACTTABLES:-True}
      APP_NVINGEST_EXTRACTCHARTS: ${APP_NVINGEST_EXTRACTCHARTS:-True}
      APP_NVINGEST_EXTRACTIMAGES: ${APP_NVINGEST_EXTRACTIMAGES:-False}
      APP_NVINGEST_EXTRACTMETHOD: ${APP_NVINGEST_EXTRACTMETHOD:-pdfium}
      # Extract text by "page" only recommended for documents with pages like .pdf, .docx, etc.
      APP_NVINGEST_TEXTDEPTH: ${APP_NVINGEST_TEXTDEPTH:-page} # extract by "page" or "document"

      ##===NV-Ingest Splitting Configurations========
      APP_NVINGEST_CHUNKSIZE: ${APP_NVINGEST_CHUNKSIZE:-1024}
      APP_NVINGEST_CHUNKOVERLAP: ${APP_NVINGEST_CHUNKOVERLAP:-150}

      ##===NV-Ingest Caption Model configurations====
      APP_NVINGEST_CAPTIONMODELNAME: ${APP_NVINGEST_CAPTIONMODELNAME:-"meta/llama-3.2-11b-vision-instruct"}
      # Incase of nvidia-hosted caption model, use the endpoint url as - https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions
      APP_NVINGEST_CAPTIONENDPOINTURL: ${APP_NVINGEST_CAPTIONENDPOINTURL:-"http://vlm-ms:8000/v1/chat/completions"}

      # Choose whether to store the extracted content in the vector store for citation support
      ENABLE_CITATIONS: ${ENABLE_CITATIONS:-True}

      # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
      LOGLEVEL: ${LOGLEVEL:-INFO}

    ports:
      - "8082:8082"
    expose:
      - "8082"
    shm_size: 5gb
    profiles: ["ingest"]

  redis:
    image: "redis/redis-stack"
    ports:
      - "6379:6379"
    profiles: ["ingest"]

  nv-ingest-ms-runtime:
    image: nvcr.io/nvidia/nemo-microservices/nv-ingest:25.3.0
    cpuset: "0-15"
    volumes:
      - ${DATASET_ROOT:-./data}:/workspace/data
    ports:
      # HTTP API
      - "7670:7670"
      # Simple Broker
      - "7671:7671"
    cap_add:
      - sys_nice
    environment:
      # Audio model not used in this RAG version
      - AUDIO_GRPC_ENDPOINT=audio:50051
      - AUDIO_INFER_PROTOCOL=grpc
      - CUDA_VISIBLE_DEVICES=0
      - MAX_INGEST_PROCESS_WORKERS=${MAX_INGEST_PROCESS_WORKERS:-16}
      - EMBEDDING_NIM_MODEL_NAME=${EMBEDDING_NIM_MODEL_NAME:-${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}}
      # Incase of self-hosted embedding model, use the endpoint url as - https://integrate.api.nvidia.com/v1
      - EMBEDDING_NIM_ENDPOINT=${EMBEDDING_NIM_ENDPOINT:-${APP_EMBEDDINGS_SERVERURL-http://nemoretriever-embedding-ms:8000/v1}}
      - INGEST_LOG_LEVEL=DEFAULT
      - INGEST_EDGE_BUFFER_SIZE=64
      # Message client for development
      #- MESSAGE_CLIENT_HOST=0.0.0.0
      #- MESSAGE_CLIENT_PORT=7671
      #- MESSAGE_CLIENT_TYPE=simple # Configure the ingest service to use the simple broker
      # Message client for production
      - MESSAGE_CLIENT_HOST=redis
      - MESSAGE_CLIENT_PORT=6379
      - MESSAGE_CLIENT_TYPE=redis
      - MINIO_BUCKET=${MINIO_BUCKET:-nv-ingest}
      - MRC_IGNORE_NUMA_CHECK=1
      - NEMORETRIEVER_PARSE_HTTP_ENDPOINT=http://nemoretriever-parse:8000/v1/chat/completions
      - NEMORETRIEVER_PARSE_INFER_PROTOCOL=http
      - NVIDIA_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - NGC_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - NVIDIA_BUILD_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317
      # Self-hosted paddle endpoints.
      - PADDLE_GRPC_ENDPOINT=paddle:8001
      - PADDLE_HTTP_ENDPOINT=${PADDLE_HTTP_ENDPOINT:-http://paddle:8000/v1/infer}
      - PADDLE_INFER_PROTOCOL=${PADDLE_INFER_PROTOCOL:-grpc}
      # build.nvidia.com hosted paddle endpoints.
      #- PADDLE_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/baidu/paddleocr
      #- PADDLE_INFER_PROTOCOL=http
      - READY_CHECK_ALL_COMPONENTS=False
      - REDIS_MORPHEUS_TASK_QUEUE=morpheus_task_queue
      # Self-hosted redis endpoints.
      # build.nvidia.com hosted yolox endpoints.
      #- YOLOX_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2
      #- YOLOX_INFER_PROTOCOL=http
      - YOLOX_GRPC_ENDPOINT=page-elements:8001
      - YOLOX_HTTP_ENDPOINT=${YOLOX_HTTP_ENDPOINT:-http://page-elements:8000/v1/infer}
      - YOLOX_INFER_PROTOCOL=${YOLOX_INFER_PROTOCOL:-grpc}
      # build.nvidia.com hosted yolox-graphics-elements endpoints.
      #- YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1
      #- YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL=http
      - YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT=graphic-elements:8001
      - YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT=${YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT:-http://graphic-elements:8000/v1/infer}
      - YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL=${YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL:-grpc}
      # build.nvidia.com hosted  yolox-table-elements endpoints.
      #- YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1
      #- YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL=http
      - YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT=table-structure:8001
      - YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT=${YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT:-http://table-structure:8000/v1/infer}
      - YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL=${YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL:-grpc}
      # Incase of nvidia-hosted caption model, use the endpoint url as - https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions
      - VLM_CAPTION_ENDPOINT=${VLM_CAPTION_ENDPOINT:-http://vlm-ms:8000/v1/chat/completions}
      - VLM_CAPTION_MODEL_NAME=${VLM_CAPTION_MODEL_NAME:-meta/llama-3.2-11b-vision-instruct}
      - MODEL_PREDOWNLOAD_PATH=${MODEL_PREDOWNLOAD_PATH:-/workspace/models/}
    healthcheck:
      test: curl --fail http://nv-ingest-ms-runtime:7670/v1/health/ready || exit 1
      interval: 10s
      timeout: 5s
      retries: 20
    profiles: ["ingest"]

  # Main orchestrator server which stiches together all calls to different services to fulfill the user request
  rag-server:
    container_name: rag-server
    image: nvcr.io/nvidia/blueprint/rag-server:${TAG:-2.0.0}
    build:
      # Set context to repo's root directory
      context: ../
      dockerfile: src/Dockerfile
    # start the server on port 8081 with 8 workers for improved latency on concurrent requests.
    command: --port 8081 --host 0.0.0.0 --workers 8

    # Common customizations to the pipeline can be controlled using env variables
    environment:
      # Path to example directory relative to root
      EXAMPLE_PATH: 'src/'

      ##===MINIO specific configurations which is used to store the multimodal base64 content===
      MINIO_ENDPOINT: "minio:9010"
      MINIO_ACCESSKEY: "minioadmin"
      MINIO_SECRETKEY: "minioadmin"

      ##===Vector DB specific configurations===
      # URL on which vectorstore is hosted
      APP_VECTORSTORE_URL: "http://milvus:19530"
      # Type of vectordb used to store embedding supported type milvus
      APP_VECTORSTORE_NAME: "milvus"
      # Type of vectordb search to be used
      APP_VECTORSTORE_SEARCHTYPE: ${APP_VECTORSTORE_SEARCHTYPE:-"dense"} # Can be dense or hybrid
      # vectorstore collection name to store embeddings
      COLLECTION_NAME: ${COLLECTION_NAME:-multimodal_data}
      APP_RETRIEVER_SCORETHRESHOLD: 0.25
      # Top K from vector DB, which goes as input to reranker model - not applicable if ENABLE_RERANKER is set to False
      VECTOR_DB_TOPK: 100

      ##===LLM Model specific configurations===
      APP_LLM_MODELNAME: ${APP_LLM_MODELNAME:-"meta/llama-3.1-70b-instruct"}
      # url on which llm model is hosted. If Nvidia hosted API is used
      APP_LLM_SERVERURL: ${APP_LLM_SERVERURL-"nim-llm:8000"}

      ##===Query Rewriter Model specific configurations===
      APP_QUERYREWRITER_MODELNAME: ${APP_QUERYREWRITER_MODELNAME:-"meta/llama-3.1-8b-instruct"}
      # url on which query rewriter model is hosted. If Nvidia hosted API is used
      APP_QUERYREWRITER_SERVERURL: ${APP_QUERYREWRITER_SERVERURL-"nim-llm-llama-8b-ms:8000"}

      ##===Embedding Model specific configurations===
      # url on which embedding model is hosted. If Nvidia hosted API is used
      APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL-"nemoretriever-embedding-ms:8000"}
      APP_EMBEDDINGS_MODELNAME: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}

      ##===Reranking Model specific configurations===
      # url on which ranking model is hosted. If Nvidia hosted API is used
      APP_RANKING_SERVERURL: ${APP_RANKING_SERVERURL-"nemoretriever-ranking-ms:8000"}
      APP_RANKING_MODELNAME: ${APP_RANKING_MODELNAME:-"nvidia/llama-3.2-nv-rerankqa-1b-v2"}
      ENABLE_RERANKER: ${ENABLE_RERANKER:-True}

      NVIDIA_API_KEY: ${NVIDIA_API_KEY:?"NVIDIA_API_KEY is required"}

      # Number of document chunks to insert in LLM prompt
      APP_RETRIEVER_TOPK: 10

      # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
      LOGLEVEL: ${LOGLEVEL:-INFO}

      # enable multi-turn conversation in the rag chain - this controls conversation history usage
      # while doing query rewriting and in LLM prompt
      ENABLE_MULTITURN: ${ENABLE_MULTITURN:-True}

      # enable query rewriting for multiturn conversation in the rag chain.
      # This will improve accuracy of the retrieiver pipeline but increase latency due to an additional LLM call
      ENABLE_QUERYREWRITER: ${ENABLE_QUERYREWRITER:-False}

      # Choose whether to enable citations in the response
      ENABLE_CITATIONS: ${ENABLE_CITATIONS:-True}

      # Choose whether to enable/disable guardrails
      ENABLE_GUARDRAILS: ${ENABLE_GUARDRAILS:-False}

      # NeMo Guardrails URL when ENABLE_GUARDRAILS is true
      NEMO_GUARDRAILS_URL: ${NEMO_GUARDRAILS_URL:-nemo-guardrails-microservice:7331}

      # number of last n chat messages to consider from the provided conversation history
      CONVERSATION_HISTORY: 5

      # Tracing
      APP_TRACING_ENABLED: "False"
      # HTTP endpoint
      APP_TRACING_OTLPHTTPENDPOINT: http://otel-collector:4318/v1/traces
      # GRPC endpoint
      APP_TRACING_OTLPGRPCENDPOINT: grpc://otel-collector:4317

      # Choose whether to enable source metadata in document content during generation
      ENABLE_SOURCE_METADATA: ${ENABLE_SOURCE_METADATA:-true}

      # Whether to filter content within <think></think> tags in model responses
      FILTER_THINK_TOKENS: ${FILTER_THINK_TOKENS:-true}

      # enable reflection (context relevance and response groundedness checking) in the rag chain
      ENABLE_REFLECTION: ${ENABLE_REFLECTION:-false}
      # Maximum number of context relevance loop iterations
      MAX_REFLECTION_LOOP: ${MAX_REFLECTION_LOOP:-3}
      # Minimum relevance score threshold (0-2)
      CONTEXT_RELEVANCE_THRESHOLD: ${CONTEXT_RELEVANCE_THRESHOLD:-1}
      # Minimum groundedness score threshold (0-2)
      RESPONSE_GROUNDEDNESS_THRESHOLD: ${RESPONSE_GROUNDEDNESS_THRESHOLD:-1}
      # reflection llm
      REFLECTION_LLM: ${REFLECTION_LLM:-"mistralai/mixtral-8x22b-instruct-v0.1"}
      # reflection llm server url. If Nvidia hosted API is used
      REFLECTION_LLM_SERVERURL: ${REFLECTION_LLM_SERVERURL-"nim-llm-mixtral-8x22b:8000"}

    ports:
      - "8081:8081"
    expose:
      - "8081"
    shm_size: 5gb
    profiles: ["rag"]

  # Sample UI container which interacts with APIs exposed by rag-server container
  rag-playground:
    container_name: rag-playground
    image: nvcr.io/nvidia/blueprint/rag-playground:${TAG:-2.0.0}
    build:
      # Set context to repo's root directory
      context: ../frontend
      dockerfile: ./Dockerfile
      args:
        # Model name for LLM
        NEXT_PUBLIC_MODEL_NAME: ${APP_LLM_MODELNAME:-meta/llama-3.1-70b-instruct}
        # Model name for embeddings
        NEXT_PUBLIC_EMBEDDING_MODEL: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}
        # Model name for reranking
        NEXT_PUBLIC_RERANKER_MODEL: ${APP_RANKING_MODELNAME:-nvidia/llama-3.2-nv-rerankqa-1b-v2}
        # URL for rag server container
        NEXT_PUBLIC_CHAT_BASE_URL: "http://rag-server:8081/v1"
        # URL for ingestor container
        NEXT_PUBLIC_VDB_BASE_URL: "http://ingestor-server:8082/v1"
    ports:
      - "8090:3000"
    expose:
      - "3000"
    depends_on:
      - rag-server
    environment:
      - NVWB_TRIM_PREFIX=true
    profiles: ["rag"]

  # Milvus can be made GPU accelerated by uncommenting the lines as specified below
  milvus:
    container_name: milvus-standalone
    image: milvusdb/milvus:v2.5.3-gpu # milvusdb/milvus:v2.5.3 for CPU
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9010
      KNOWHERE_GPU_MEM_POOL_SIZE: 2048;4096
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-./volumes/milvus}:/var/lib/milvus
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
    #   interval: 30s
    #   start_period: 90s
    #   timeout: 20s
    #   retries: 3
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"
    # Comment out this section if CPU based image is used and set below env variables to False
    # export APP_VECTORSTORE_ENABLEGPUSEARCH=False
    # export APP_VECTORSTORE_ENABLEGPUINDEX=False
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              # count: ${INFERENCE_GPU_COUNT:-all}
              device_ids: ['${VECTORSTORE_GPU_DEVICE_ID:-4}']
    profiles: ["vectordb"]

  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.19
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-./volumes/etcd}:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3
    profiles: ["vectordb"]

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2025-02-28T09-55-16Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9011:9011"
      - "9010:9010"
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-./volumes/minio}:/minio_data
    command: minio server /minio_data --console-address ":9011" --address ":9010"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    profiles: ["vectordb"]

volumes:
  ingest:
  vectordb:
  nim_cache:
    external: true

networks:
  default:
    name: nvidia-rag
